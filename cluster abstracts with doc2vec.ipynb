{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster abstracts with doc2vec\n",
    "Tested under Python 3.7.<br>\n",
    "First run <i>convert all pdfs to dfs</i>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for training, either from single set of abstracts or from multiple sets combined into one dataframe \n",
    "train_df_path = r''\n",
    "\n",
    "# the one dataframe that should be clustered (or directory, if all dataframes in that directory should be clustered)\n",
    "cluster_df_path = r''\n",
    "\n",
    "human_clusters_path = r''\n",
    "nlp_clusters_path = r''\n",
    "\n",
    "n_clusters = 20\n",
    "\n",
    "# vec_size is the dimensionality of the final vector, \n",
    "# i.e. it must be smaller than the number of documents used for training, or else all vectors can be linearly independent \n",
    "# usually vec_size is tens to hundreds, and number of documents is millions\n",
    "# vector_size = n_clusters # this is probably a good setting if training set is very small (100 abstracts)\n",
    "\n",
    "vector_size = 10\n",
    "\n",
    "title_weight = 3 # weight of title (w.r.t. rest of text), must be integer\n",
    "\n",
    "custom_filter = ['could', 'might', 'many', 'also', 'scan', 'abstracts', 'use', 'people', 'new', 'researchers', 'would', 'may', 'one', 'users', 'article', 'using']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nint(i):\n",
    "    return int(round(i))\n",
    "\n",
    "def timestamp():\n",
    "    import time\n",
    "    return time.strftime('_%Y%m%d_%H%M%S')\n",
    "\n",
    "def print_time(time0 = 0):\n",
    "    import time\n",
    "    print(\"Time elapsed: \", (time.time()-time0)/60., \" min\")\n",
    "    \n",
    "def remove_stopwords(words):\n",
    "    \"\"\"\n",
    "    input: list\n",
    "    output: list without stopwords, all entries are cast to lower case\n",
    "    \"\"\"\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if (word.lower() not in stopwords.words('english')) & (word.lower() not in custom_filter):\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "def assure_index(df):\n",
    "    if df.index.name != 'abstract_id':\n",
    "        df.set_index('abstract_id', inplace = True) \n",
    "    df['abstract_id'] = df.index\n",
    "    return df\n",
    "\n",
    "def make_text_col(df, title_weight = 3):\n",
    "    \"\"\"\n",
    "    combine abstract title, overview, summary, implications into one text\n",
    "    input: df\n",
    "    output: df with additional column 'text'\n",
    "    \"\"\"\n",
    "    df['text'] = (df.abstract_title + ' ') * nint(title_weight) + ' ' + df.overview + ' ' + df.summary + ' ' + df.implications\n",
    "    return df\n",
    "\n",
    "def tag_data(df_text, rm_stopwords=True):\n",
    "    \"\"\"\n",
    "    takes an array of text, removes stopwords, tags each element\n",
    "    input: column of df containing text\n",
    "    output: list of TaggedDocuments, each TaggedDocument is like a dict with an array of words and a tag, specific type for gensim.models.doc2vec\n",
    "    \"\"\"\n",
    "    tagged_data = []\n",
    "    for i, doc in enumerate(df_text):\n",
    "#         filtered_words = [word for word in word_tokenize(doc.lower()) if word not in stopwords.words('english')]\n",
    "        if rm_stopwords == True:\n",
    "            filtered_words = remove_stopwords(word_tokenize(doc))\n",
    "        else:\n",
    "            filtered_words = word_tokenize(doc)\n",
    "        tagged_data.append(TaggedDocument(words=filtered_words, tags=[str(i)]))\n",
    "    return tagged_data\n",
    "\n",
    "def make_index(a, prefix):\n",
    "    a2 = np.zeros_like(a, dtype=object)\n",
    "    for i in range(len(a)):\n",
    "        a2[i] = prefix+str(a[i]).zfill(3)\n",
    "    return a2\n",
    "\n",
    "def id_to_int(id):\n",
    "    return int(id[-3:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataframes\n",
    "Read training data and abstracts to be clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(train_df_path)\n",
    "df = assure_index(df)\n",
    "print('train df head:')\n",
    "display(df.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.read_pickle(cluster_df_path)\n",
    "df_cluster = assure_index(df_cluster)\n",
    "prefix = np.array(Counter(df_cluster.abstract_id.apply(lambda x: x[:14])).most_common())[0][0]\n",
    "df_cluster = make_text_col(df_cluster, title_weight = title_weight)\n",
    "print('cluster df head:')\n",
    "display(df_cluster.head())\n",
    "print(len(df_cluster))\n",
    "print('prefix: ', prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create column that holds all text to be transformed to vector\n",
    "Title has (potentially) higher weight, see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_text_col(df, title_weight = title_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag training data\n",
    "To make format compatible with gensim implementation of doc2vec.\n",
    "Inspired by https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "# new version, removes stopwords\n",
    "# takes 5 min for all training data\n",
    "tagged_data = tag_data(df.text)\n",
    "print_time(time0)\n",
    "\n",
    "save_tagged_data(tagged_data, 'onetagperabstract_title'+str(int(title_weight))+'_nostopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tagged data (or store tagged data)\n",
    "To avoid tagging again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tagged_data))\n",
    "\n",
    "def save_tagged_data(tagged_data, name=''):\n",
    "    savepath = os.path.join(os.path.dirname(train_df_path),'tagged_data_'+name+timestamp()+'.data')\n",
    "    print('Tagged data saved as ', savepath)\n",
    "    f = open(savepath, 'wb')\n",
    "    pickle.dump(tagged_data, f)\n",
    "    f.close()\n",
    "\n",
    "def read_tagged_data(name=''):\n",
    "    if name == '':\n",
    "        name = glob.glob(os.path.join(os.path.dirname(train_df_path),'tagged_data_*.data'))[-1]\n",
    "    else:\n",
    "        name = glob.glob(os.path.join(os.path.dirname(train_df_path),'*'+name+'*.data'))[-1]\n",
    "    print('Reading tagged data from ', name)\n",
    "    return pickle.load(open(name, 'rb'))\n",
    "\n",
    "tagged_data = read_tagged_data('yourfilenamehere')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "vector_size = vector_size \n",
    "alpha = 0.025\n",
    "\n",
    "time0 = time.time()\n",
    "model = Doc2Vec(vector_size=vector_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm =0)  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "# model.save(os.path.join(os.path.dirname(train_df_path),'d2v_onerowpersentence'+timestamp()+'.model'))\n",
    "savepath = os.path.join(os.path.dirname(train_df_path),'d2v'+timestamp()+'.model')\n",
    "model.save(savepath)\n",
    "print(\"Model saved as \", savepath)\n",
    "print_time(time0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = r'your .model path here'\n",
    "print(\"Loading model \", model)\n",
    "model= Doc2Vec.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add fake entry to see if it is assigned to same cluster (for quality control only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fake_entry(df):\n",
    "    \"\"\"\n",
    "    adds a fake row into df, similar to existing row\n",
    "    this is to check if they end up in the same cluster\n",
    "    \"\"\"\n",
    "    df = df.append(df.iloc[-1,:], ignore_index=True)\n",
    "    df.iloc[-1, df.columns.get_loc('abstract_id')] = df.iloc[-1, df.columns.get_loc('abstract_id')].replace('SC-', 'fak')\n",
    "    text = df.iloc[-1, df.columns.get_loc('text')]\n",
    "    text = 'fakefakefake' + text[12:]\n",
    "    df.iloc[-1, df.columns.get_loc('text')] = text\n",
    "    return df\n",
    "\n",
    "df_fake = add_fake_entry(df_cluster)\n",
    "df_fake.tail()\n",
    "df_cluster = df_fake\n",
    "tagged_data = tag_data(df_cluster.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model to data to be clustered (assign vector to each abstract), cluster vectors\n",
    "See below to apply this to a bunch of abstract sets at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from nltk.cluster import KMeansClusterer\n",
    "\n",
    "# Tag data to be clustered\n",
    "df_cluster = make_text_col(df_cluster)\n",
    "tagged_data = tag_data(df_cluster.text)\n",
    "\n",
    "# Assign vector to each abstract\n",
    "X = np.zeros((len(df_cluster), vector_size))\n",
    "for i in range(len(df_cluster)):\n",
    "    print('Inference for ', df_cluster.iloc[i, df_cluster.columns.get_loc('abstract_title')])\n",
    "    X[i,:] = model.infer_vector(tagged_data[i].words)\n",
    "    if i == len(df_cluster)-1:\n",
    "        break\n",
    "\n",
    "\n",
    "# kclusterer = KMeansClusterer(n_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "kclusterer = KMeansClusterer(n_clusters, distance=nltk.cluster.util.euclidean_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "print(len(assigned_clusters))\n",
    "print(assigned_clusters)\n",
    "df_cluster['assigned_cluster'] = assigned_clusters\n",
    "# assigned_clusters = df.index[assigned_clusters].values\n",
    "# print(assigned_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print clusters to screen and to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "#  sometimes has to be run twice to avoid error, not sure why\n",
    "\n",
    "min_size = 3 # minimum number of abstract in a cluster to be printed to file\n",
    "\n",
    "\n",
    "def get_most_frequent_from_df(dfp, n=5):\n",
    "    \"\"\"\n",
    "    input: a dataframe and number n of most frequent words desired\n",
    "    output: \n",
    "    the most common n words from all of the fields in the dataframe\n",
    "    np array with n rows, column zero has frequent words, column 1 has number of word occurences, row 0 has most frequent word\n",
    "    This is used to generate a title for each cluster.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cols = dfp.columns.values\n",
    "    dfp['text_all'] = dfp.loc[:, cols[0]]\n",
    "    try:\n",
    "        for col in cols[1:]:\n",
    "            dfp.loc[:, 'text_all'] += dfp.loc[:, col].values\n",
    "    except:\n",
    "        pass\n",
    "    text_all = remove_stopwords(str(dfp['text_all'].values).lower().split())\n",
    "    return np.array(Counter(text_all).most_common(n))\n",
    "\n",
    "nlp_clusters = []\n",
    "\n",
    "colidx = []\n",
    "for col in ['abstract_title', 'overview', 'summary', 'implications']:\n",
    "    colidx.append(df_cluster.columns.get_loc(col))\n",
    "\n",
    "print(n_clusters, ' Clusters:')    \n",
    "for i in range(n_clusters):\n",
    "    idx = np.where(df_cluster.assigned_cluster.values == i)[0]\n",
    "    titles = df_cluster.iloc[idx, df_cluster.columns.get_loc('abstract_title')].values\n",
    "    if np.sum(idx) >= min_size:\n",
    "        indices = df_cluster.index.values[idx]    \n",
    "        authors = df_cluster.submitted_by.values[idx]\n",
    "        most_frequent = get_most_frequent_from_df(df_cluster.iloc[idx, colidx])[:,0]\n",
    "        print('Cluster number ', str(i), ': ', most_frequent)\n",
    "        for index, author, title in zip(indices, authors, titles):\n",
    "            print(index, \": (\", author, \")\", title)\n",
    "        nlp_clusters.append(most_frequent)\n",
    "        if float(np.array(Counter(authors).most_common(1))[0][1])/len(authors) >= .5:\n",
    "            print('More than or equal to half of the abstracts in this cluster were submitted by the same person.')\n",
    "        print('____________________________________________________________________________')\n",
    "txt = cluster_df_path.replace('.df', '_NLP_clusters_1d_'+timestamp()+'.txt')\n",
    "with open(txt, 'w', encoding='utf-8') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('saved to ', txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model to a series of abstract sets, save as dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from nltk.cluster import KMeansClusterer\n",
    "\n",
    "def load_df_tagged(dfname):\n",
    "    df_cluster = pd.read_pickle(dfname)\n",
    "    df_cluster = assure_index(df_cluster)\n",
    "    try:\n",
    "        name = dfname.replace('.df', '.data')\n",
    "        tagged_data = pickle.load(open(name, 'rb'))\n",
    "        print('Tagged data read from ', name)\n",
    "    except:\n",
    "        print('Reading ', dfname)\n",
    "        prefix = np.array(Counter(df_cluster.abstract_id.apply(lambda x: x[:14])).most_common())[0][0]\n",
    "        print('prefix: ', prefix)\n",
    "        # Tag data to be clustered\n",
    "        df_cluster = make_text_col(df_cluster)\n",
    "        print('tagging ... please wait')\n",
    "        tagged_data = tag_data(df_cluster.text)\n",
    "        f = open(name, 'wb')\n",
    "        pickle.dump(tagged_data, f)\n",
    "        f.close()\n",
    "        print('Saved tagged data to ', name)\n",
    "    return [tagged_data, df_cluster]\n",
    "        \n",
    "time0 = time.time()\n",
    "\n",
    "if os.path.isdir(cluster_df_path):\n",
    "    dfnames = glob.glob(os.path.join(cluster_df_path, '*.df'))\n",
    "    for dfname in dfnames:\n",
    "        [tagged_data, df_cluster] = load_df_tagged(dfname)\n",
    "        # Assign vector to each abstract\n",
    "        X = np.zeros((len(df_cluster), vector_size))\n",
    "        for i in range(len(df_cluster)):\n",
    "            print('Inference for ', df_cluster.iloc[i, df_cluster.columns.get_loc('abstract_title')])\n",
    "            X[i,:] = model.infer_vector(tagged_data[i].words)\n",
    "            if i == len(df_cluster)-1:\n",
    "                break\n",
    "        \n",
    "        # cluster vectors\n",
    "#         kclusterer = KMeansClusterer(n_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "        kclusterer = KMeansClusterer(n_clusters, distance=nltk.cluster.util.euclidean_distance, repeats=25)\n",
    "        assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "        # print(len(assigned_clusters))\n",
    "        # print(assigned_clusters)\n",
    "        df_cluster['assigned_cluster'] = assigned_clusters\n",
    "        df_cluster.to_pickle(os.path.join(cluster_df_path, 'clusters_NLP', os.path.basename(dfname).replace('.df', '_clusters_NLP.df')))\n",
    "else:\n",
    "    print('cluster_df_path is not a directory')\n",
    "    \n",
    "print_time(time0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Similarity of clusters: Rand Index\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html<br>\n",
    "Nicely explained:<br>\n",
    "https://davetang.org/muse/2017/09/21/the-rand-index/\n",
    "https://davetang.org/muse/2017/09/21/adjusted-rand-index/<br>\n",
    "Rand Index is invariable to permutation.\n",
    "Only problem is that these metrics assume non-overlapping clusters, but the Scan ground truth has overlapping clusters, i.e. the same abstract can be assigned to multiple clusters. Modifying the above algorithm to account for that does not seem to be easy, given that there are so many new options now.<br>\n",
    "Maybe the best way to tackle this is to assign each abstract to exactly one cluster. If SBI has it assigned to multiple clusters, pick one of them randomly. Maybe repeat multiple times and see how the random pick affects Rand Index.\n",
    "<br>\n",
    "Paper that deals with measuring similarity between overlapping clusters: C:\\python\\scansmeeting\\feb2019\\Measuring Similarity between Sets of Overlapping Clusters.pdf<br>Paper contains three proposed algorithms, I could not find evidence that the community has agrees that either of them is standard.<br>\n",
    "<b>Rand Index: <br></b>\n",
    "1.0: clusters are the same<br>\n",
    "0.0: clusters are randomly related<br>\n",
    "< 0 :clusters are opposite<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between NLP clusters and SBI ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "dfnames_nlp = glob.glob(os.path.join(nlp_clusters_path, '*.df'))\n",
    "max_similarities = np.zeros(len(dfnames_nlp))\n",
    "avg_similarities = np.zeros(len(dfnames_nlp))\n",
    "for j, dfname_nlp in enumerate(dfnames_nlp):\n",
    "    print(dfname_nlp)\n",
    "    df_nlp = pd.read_pickle(dfname_nlp)\n",
    "    if id_to_int(df_nlp.abstract_id.values[-1]) != len(df_nlp):\n",
    "        print('Missing abstract ID in NLP')\n",
    "    else:\n",
    "        nlp_clusters = np.array(df_nlp.assigned_cluster.values)\n",
    "    root = os.path.basename(dfname_nlp)[7:17]\n",
    "    print(root)\n",
    "    dfnames_human = glob.glob(os.path.join(human_clusters_path, '*'+root+'*.df'))\n",
    "    similarities = np.zeros(len(dfnames_human))\n",
    "    for i, dfname_human in enumerate(dfnames_human):\n",
    "        df_human = pd.read_pickle(dfname_human)\n",
    "        if df_human['Abstract number'].values[-1] != len(df_human):\n",
    "            print('Missing abstract ID in human')\n",
    "        elif len(df_nlp) > len(df_human):\n",
    "            # human_clusters can be shorter, because if the last abstract(s) was not assigned to a cluster in the report, \n",
    "            # we don't know that the abstract even existed\n",
    "            # => append human_clusters with new cluster labels\n",
    "            n_missing = len(df_nlp) - len(df_human)\n",
    "            human_clusters = np.array(df_human['Cluster ID'].values)\n",
    "            human_clusters = np.append(human_clusters, np.arange(n_missing)+1+np.max(human_clusters))\n",
    "            print('Fixed human clusters. Added ', n_missing, ' abstract(s) and cluster(s).')\n",
    "        else:\n",
    "            human_clusters = np.array(df_human['Cluster ID'].values)\n",
    "        try:\n",
    "            similarities[i] = adjusted_rand_score(nlp_clusters, human_clusters)\n",
    "            print('Similarity: ', similarities[i])\n",
    "        except:\n",
    "            print('Could not compute similarity.', len(nlp_clusters), len(human_clusters))\n",
    "        \n",
    "#         break\n",
    "    print('Average similarity, max similarity: ', np.average(similarities), np.max(similarities))\n",
    "    max_similarities[j] = np.max(similarities)\n",
    "    avg_similarities[j] = np.average(similarities)\n",
    "    print(dfname_nlp)\n",
    "    print(dfnames_human)\n",
    "    print('_____________________________')\n",
    "#     break\n",
    "plt.hist(max_similarities, color='black', alpha =.5, bins=40, label='max')\n",
    "plt.hist(avg_similarities, color='red', alpha =.5, bins=40, label='avg')\n",
    "plt.legend()\n",
    "plt.xlabel('Similarity')\n",
    "plt.ylabel('Number of abstract sets')\n",
    "plt.show()\n",
    "print('Average max. similarity: ', np.average(max_similarities))\n",
    "print('Average avg. similarity: ', np.average(avg_similarities))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scansmeeting_py37",
   "language": "python",
   "name": "scansmeeting_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
